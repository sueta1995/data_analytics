{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c70ce87",
   "metadata": {},
   "source": [
    "# Лабораторная работа 1. Прогнозирование моделью линейной регрессии\n",
    "\n",
    "### 0. Загрузка данных\n",
    "\n",
    "Перед нормированием и расчетом весов необходимо загрузить данные с помощью библиотеки numpy (```X``` - матрица признаков, ```y``` - матрица целевых значений)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "66b95a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрицы X и y:\n",
      "[[7.000e+00 1.590e+01 8.200e+00 5.100e+00 1.380e+01 2.290e+02 1.720e+02\n",
      "  2.000e+01 2.104e+03]\n",
      " [7.200e+00 1.820e+01 7.400e+00 6.100e+00 1.430e+01 1.460e+02 1.670e+02\n",
      "  2.910e+01 2.489e+03]\n",
      " [7.900e+00 1.970e+01 6.400e+00 4.700e+00 1.980e+01 1.740e+02 1.440e+02\n",
      "  2.280e+01 2.428e+03]\n",
      " [7.700e+00 2.080e+01 6.900e+00 5.200e+00 1.710e+01 1.280e+02 1.110e+02\n",
      "  4.270e+01 2.494e+03]\n",
      " [9.200e+00 1.590e+01 7.800e+00 5.300e+00 1.670e+01 1.690e+02 1.480e+02\n",
      "  2.270e+01 2.094e+03]\n",
      " [7.600e+00 1.640e+01 6.700e+00 4.700e+00 1.550e+01 1.440e+02 1.500e+02\n",
      "  2.790e+01 1.768e+03]\n",
      " [7.300e+00 1.830e+01 6.300e+00 4.900e+00 1.960e+01 1.380e+02 1.330e+02\n",
      "  3.370e+01 1.982e+03]\n",
      " [7.900e+00 1.640e+01 6.800e+00 5.000e+00 1.760e+01 1.970e+02 1.550e+02\n",
      "  2.660e+01 1.621e+03]\n",
      " [7.900e+00 1.700e+01 6.300e+00 4.400e+00 2.010e+01 1.820e+02 1.590e+02\n",
      "  3.050e+01 1.631e+03]\n",
      " [8.000e+00 1.690e+01 8.200e+00 4.600e+00 1.550e+01 5.200e+02 1.970e+02\n",
      "  1.910e+01 1.066e+03]\n",
      " [7.200e+00 1.760e+01 8.100e+00 5.200e+00 1.610e+01 1.430e+02 1.650e+02\n",
      "  3.120e+01 1.183e+03]\n",
      " [8.700e+00 1.600e+01 7.600e+00 4.400e+00 1.890e+01 2.140e+02 1.610e+02\n",
      "  2.270e+01 1.308e+03]\n",
      " [7.800e+00 1.790e+01 7.200e+00 4.300e+00 1.570e+01 1.580e+02 1.630e+02\n",
      "  2.440e+01 1.475e+03]\n",
      " [8.000e+00 1.690e+01 6.900e+00 4.700e+00 1.680e+01 1.850e+02 1.460e+02\n",
      "  1.980e+01 2.081e+03]\n",
      " [7.500e+00 1.940e+01 6.700e+00 4.600e+00 1.930e+01 1.530e+02 1.650e+02\n",
      "  2.860e+01 2.109e+03]\n",
      " [7.300e+00 1.940e+01 7.400e+00 5.000e+00 2.010e+01 2.000e+02 1.750e+02\n",
      "  1.620e+01 1.757e+03]\n",
      " [7.600e+00 1.730e+01 7.100e+00 5.300e+00 1.200e+01 1.800e+02 1.540e+02\n",
      "  2.130e+01 2.111e+03]\n",
      " [9.600e+00 1.300e+01 6.400e+00 3.500e+00 1.680e+01 1.200e+02 1.170e+02\n",
      "  4.320e+01 2.112e+03]\n",
      " [9.000e+00 1.410e+01 7.000e+00 3.300e+00 1.520e+01 1.320e+02 1.260e+02\n",
      "  3.470e+01 1.794e+03]\n",
      " [1.020e+01 1.300e+01 7.100e+00 3.200e+00 1.610e+01 1.450e+02 1.210e+02\n",
      "  2.730e+01 1.688e+03]\n",
      " [8.100e+00 1.630e+01 6.200e+00 3.900e+00 1.710e+01 1.370e+02 1.210e+02\n",
      "  3.200e+01 1.774e+03]\n",
      " [8.000e+00 1.750e+01 6.700e+00 4.000e+00 1.640e+01 1.810e+02 1.820e+02\n",
      "  2.200e+01 1.773e+03]\n",
      " [1.070e+01 1.440e+01 8.000e+00 4.000e+00 1.870e+01 1.290e+02 1.300e+02\n",
      "  4.630e+01 1.344e+03]\n",
      " [2.180e+01 7.500e+00 6.900e+00 1.300e+00 1.760e+01 8.600e+01 7.900e+01\n",
      "  4.150e+01 6.730e+02]\n",
      " [1.370e+01 1.040e+01 7.100e+00 3.400e+00 1.450e+01 1.280e+02 1.020e+02\n",
      "  4.250e+01 8.590e+02]\n",
      " [1.290e+01 1.030e+01 7.000e+00 3.300e+00 1.630e+01 1.230e+02 1.070e+02\n",
      "  4.570e+01 9.250e+02]\n",
      " [1.330e+01 1.300e+01 6.600e+00 2.600e+00 1.780e+01 1.280e+02 1.010e+02\n",
      "  4.280e+01 9.680e+02]\n",
      " [1.000e+01 1.530e+01 8.800e+00 5.000e+00 1.920e+01 1.750e+02 1.600e+02\n",
      "  3.240e+01 1.565e+03]\n",
      " [1.070e+01 1.350e+01 8.100e+00 4.500e+00 2.170e+01 1.510e+02 1.540e+02\n",
      "  3.960e+01 1.325e+03]\n",
      " [9.200e+00 1.580e+01 8.000e+00 4.800e+00 1.870e+01 1.460e+02 1.400e+02\n",
      "  3.340e+01 1.497e+03]\n",
      " [1.120e+01 1.270e+01 7.300e+00 3.700e+00 1.830e+01 1.580e+02 1.910e+02\n",
      "  3.240e+01 1.059e+03]\n",
      " [9.400e+00 1.370e+01 6.700e+00 3.400e+00 1.840e+01 1.580e+02 1.440e+02\n",
      "  2.610e+01 1.915e+03]\n",
      " [9.000e+00 1.460e+01 7.300e+00 4.300e+00 2.260e+01 1.130e+02 1.320e+02\n",
      "  5.040e+01 2.660e+03]\n",
      " [1.030e+01 1.350e+01 7.500e+00 4.000e+00 1.970e+01 1.150e+02 1.450e+02\n",
      "  4.930e+01 1.534e+03]\n",
      " [9.200e+00 1.580e+01 5.900e+00 3.800e+00 1.890e+01 1.840e+02 1.750e+02\n",
      "  2.570e+01 2.654e+03]\n",
      " [8.500e+00 1.560e+01 6.700e+00 4.700e+00 1.750e+01 1.630e+02 1.690e+02\n",
      "  2.950e+01 2.508e+03]\n",
      " [9.000e+00 1.480e+01 7.000e+00 4.700e+00 1.660e+01 1.710e+02 1.820e+02\n",
      "  2.790e+01 1.987e+03]\n",
      " [1.420e+01 1.310e+01 7.100e+00 3.800e+00 2.790e+01 1.880e+02 1.480e+02\n",
      "  2.620e+01 2.176e+03]\n",
      " [8.700e+00 1.470e+01 7.300e+00 4.400e+00 2.080e+01 1.580e+02 1.460e+02\n",
      "  3.370e+01 1.871e+03]\n",
      " [8.900e+00 1.660e+01 7.000e+00 4.900e+00 1.960e+01 2.540e+02 2.600e+02\n",
      "  1.610e+01 1.563e+03]\n",
      " [8.500e+00 1.410e+01 7.000e+00 4.500e+00 1.590e+01 1.360e+02 1.560e+02\n",
      "  3.980e+01 2.665e+03]\n",
      " [1.020e+01 1.230e+01 7.300e+00 4.600e+00 1.630e+01 1.570e+02 1.700e+02\n",
      "  2.970e+01 2.273e+03]\n",
      " [9.100e+00 1.300e+01 7.000e+00 5.300e+00 2.120e+01 1.730e+02 1.900e+02\n",
      "  3.060e+01 2.635e+03]\n",
      " [1.060e+01 9.800e+00 7.900e+00 5.700e+00 2.130e+01 2.900e+02 2.930e+02\n",
      "  1.920e+01 2.478e+03]\n",
      " [1.170e+01 1.200e+01 6.500e+00 3.500e+00 1.520e+01 1.220e+02 1.550e+02\n",
      "  5.520e+01 2.580e+03]\n",
      " [2.000e+01 1.300e+01 5.900e+00 1.900e+00 2.800e+01 8.400e+01 1.010e+02\n",
      "  7.320e+01 2.713e+03]\n",
      " [9.900e+00 1.400e+01 7.100e+00 4.400e+00 2.460e+01 1.610e+02 2.010e+02\n",
      "  2.530e+01 2.222e+03]\n",
      " [9.800e+00 1.400e+01 7.200e+00 4.800e+00 1.980e+01 2.460e+02 2.960e+02\n",
      "  2.420e+01 2.417e+03]\n",
      " [1.060e+01 1.460e+01 6.300e+00 3.300e+00 1.810e+01 1.700e+02 2.150e+02\n",
      "  3.230e+01 2.317e+03]\n",
      " [1.220e+01 1.280e+01 6.900e+00 4.000e+00 2.080e+01 9.900e+01 1.120e+02\n",
      "  6.650e+01 2.784e+03]]\n",
      "[[59.9]\n",
      " [55.5]\n",
      " [55.3]\n",
      " [55.8]\n",
      " [60.1]\n",
      " [58.5]\n",
      " [57.4]\n",
      " [58.5]\n",
      " [58.3]\n",
      " [58.2]\n",
      " [56.5]\n",
      " [59.2]\n",
      " [58.1]\n",
      " [58.8]\n",
      " [56.5]\n",
      " [57.1]\n",
      " [58.3]\n",
      " [59.4]\n",
      " [61.2]\n",
      " [60.4]\n",
      " [58.6]\n",
      " [57.5]\n",
      " [60.6]\n",
      " [65.9]\n",
      " [62.6]\n",
      " [63.9]\n",
      " [59.5]\n",
      " [59.4]\n",
      " [61. ]\n",
      " [59.4]\n",
      " [60.2]\n",
      " [57.5]\n",
      " [68.9]\n",
      " [59. ]\n",
      " [56.9]\n",
      " [57.7]\n",
      " [58. ]\n",
      " [55.1]\n",
      " [58.3]\n",
      " [55.4]\n",
      " [59.1]\n",
      " [60.6]\n",
      " [58.2]\n",
      " [57.8]\n",
      " [57.2]\n",
      " [49.7]\n",
      " [56. ]\n",
      " [56. ]\n",
      " [54.7]\n",
      " [56.2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# получение данных из файлов\n",
    "X = np.genfromtxt(\"./datasets/lab1_X.csv\")\n",
    "y = np.genfromtxt(\"./datasets/lab1_y.csv\").reshape(X.shape[0], 1)\n",
    "\n",
    "print(f\"Матрицы X и y:\\n{X}\\n{y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e664d",
   "metadata": {},
   "source": [
    "Разделим все данные на выборку для обучения и выборку для валидации (```X_train```, ```y_train``` и ```X_test```, ```y_test``` соответственно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4cb2e2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерности матриц: X_test - (15, 9), X_train - (35, 9), y_test - (15, 1), y_train - (35, 1)\n"
     ]
    }
   ],
   "source": [
    "# разделяем данные так: 70% для обучения, 30% для валидации\n",
    "split = round(X.shape[0] * 0.7)\n",
    "\n",
    "# Разделение для матрицы признаков\n",
    "X_train = X[:split]\n",
    "X_test = X[split:]\n",
    "\n",
    "# Разделение для матрицы целевых значений\n",
    "y_train = y[:split]\n",
    "y_test = y[split:]\n",
    "\n",
    "print(f\"Размерности матриц: X_test - {X_test.shape}, X_train - {X_train.shape}, y_test - {y_test.shape}, y_train - {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c54c4",
   "metadata": {},
   "source": [
    "### 1. Нормирование (масштабирование) исходных данных.\n",
    "\n",
    "Теперь необходимо нормализовать данные, поскольку разброс значенией данных слишком большой. Будет использоваться Z-нормализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ee19f1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.86580164  0.12296253  1.55757652  0.91545987 -1.66258049  0.94244347\n",
      "   0.9941688  -1.24316968  0.76671086]\n",
      " [-0.79287009  0.94101901  0.35944073  2.02032522 -1.43859928 -0.27652282\n",
      "   0.80669085 -0.26936362  1.51428169]\n",
      " [-0.53760968  1.4745341  -1.13822899  0.47351372  1.02519397  0.13469473\n",
      "  -0.05570773 -0.94353704  1.39583541]\n",
      " [-0.61054123  1.86577851 -0.38939413  1.0259464  -0.18430453 -0.54087695\n",
      "  -1.29306223  1.18599488  1.52399041]\n",
      " [-0.06355463  0.12296253  0.95850863  1.13643294 -0.3634895   0.06126302\n",
      "   0.09427463 -0.95423821  0.74729344]\n",
      " [-0.647007    0.30080089 -0.68892807  0.47351372 -0.90104439 -0.3058955\n",
      "   0.16926581 -0.39777761  0.11428541]\n",
      " [-0.75640432  0.97658668 -1.28799597  0.69448679  0.93560149 -0.39401355\n",
      "  -0.46815923  0.22288999  0.52981829]\n",
      " [-0.53760968  0.30080089 -0.5391611   0.80497333  0.03967667  0.47248057\n",
      "   0.35674376 -0.53689276 -0.17115072]\n",
      " [-0.53760968  0.51420693 -1.28799597  0.14205412  1.1595827   0.25218545\n",
      "   0.50672613 -0.11954731 -0.1517333 ]\n",
      " [-0.50114391  0.47863926  1.55757652  0.36302719 -0.90104439  5.21616867\n",
      "   1.93155857 -1.33948017 -1.24881777]\n",
      " [-0.79287009  0.72761297  1.40780954  1.0259464  -0.63226694 -0.32058184\n",
      "   0.73169967 -0.04463915 -1.0216339 ]\n",
      " [-0.2458835   0.1585302   0.65897468  0.14205412  0.6220278   0.72214836\n",
      "   0.58171731 -0.95423821 -0.7789161 ]\n",
      " [-0.57407545  0.83431599  0.05990679  0.03156758 -0.81145191 -0.10028673\n",
      "   0.65670849 -0.7723184  -0.45464512]\n",
      " [-0.50114391  0.47863926 -0.38939413  0.47351372 -0.31869326  0.29624448\n",
      "   0.01928345 -1.26457201  0.72205079]\n",
      " [-0.68347277  1.36783109 -0.68892807  0.36302719  0.80121277 -0.17371843\n",
      "   0.73169967 -0.32286945  0.77641958]\n",
      " [-0.75640432  1.36783109  0.35944073  0.80497333  1.1595827   0.51653959\n",
      "   1.10665558 -1.64981396  0.09292624]\n",
      " [-0.647007    0.62090995 -0.08986018  1.13643294 -2.46891283  0.22281277\n",
      "   0.31924817 -1.10405453  0.78030306]\n",
      " [ 0.08230846 -0.90849999 -1.13822899 -0.8523247  -0.31869326 -0.65836768\n",
      "  -1.06808868  1.23950071  0.7822448 ]\n",
      " [-0.13648618 -0.51725559 -0.23962716 -1.07329777 -1.03543311 -0.48213159\n",
      "  -0.73062837  0.32990164  0.16477071]\n",
      " [ 0.3011031  -0.90849999 -0.08986018 -1.18378431 -0.63226694 -0.29120916\n",
      "  -0.91810632 -0.4619846  -0.04105398]\n",
      " [-0.46467813  0.26523322 -1.43776294 -0.41037856 -0.18430453 -0.40869989\n",
      "  -0.91810632  0.04097018  0.12593587]\n",
      " [-0.50114391  0.6920453  -0.68892807 -0.29989203 -0.49787822  0.23749911\n",
      "   1.36912471 -1.02914637  0.12399412]\n",
      " [ 0.48343196 -0.41055257  1.25804257 -0.29989203  0.53243532 -0.52619061\n",
      "  -0.580646    1.57123683 -0.70901337]\n",
      " [ 4.53113275 -2.86472201 -0.38939413 -3.28302849  0.03967667 -1.15770327\n",
      "  -2.49292113  1.05758089 -2.01192254]\n",
      " [ 1.57740515 -1.83325949 -0.08986018 -0.96281124 -1.3490068  -0.54087695\n",
      "  -1.63052254  1.16459255 -1.65075845]\n",
      " [ 1.28567896 -1.86882717 -0.23962716 -1.07329777 -0.54267446 -0.61430866\n",
      "  -1.44304459  1.50702984 -1.52260345]\n",
      " [ 1.43154205 -0.90849999 -0.83869505 -1.84670352  0.12926915 -0.54087695\n",
      "  -1.66801813  1.19669604 -1.43910852]\n",
      " [ 0.22817155 -0.09044351  2.45617835  0.80497333  0.75641653  0.14938107\n",
      "   0.54422172  0.08377484 -0.2798883 ]\n",
      " [ 0.48343196 -0.73066163  1.40780954  0.25254065  1.87632255 -0.20309111\n",
      "   0.31924817  0.85425875 -0.74590648]\n",
      " [-0.06355463  0.08739485  1.25804257  0.58400026  0.53243532 -0.27652282\n",
      "  -0.2056901   0.19078649 -0.41192678]\n",
      " [ 0.66576082 -1.01520301  0.20967376 -0.63135163  0.35325036 -0.10028673\n",
      "   1.70658503  0.08377484 -1.26240996]\n",
      " [ 0.00937691 -0.65952628 -0.68892807 -0.96281124  0.3980466  -0.10028673\n",
      "  -0.05570773 -0.59039858  0.39972155]\n",
      " [-0.13648618 -0.33941722  0.20967376  0.03156758  2.27948872 -0.76117207\n",
      "  -0.50565482  2.00998462  1.84631965]\n",
      " [ 0.33756887 -0.73066163  0.50920771 -0.29989203  0.98039773 -0.73179939\n",
      "  -0.01821214  1.8922718  -0.34008231]\n",
      " [-0.06355463  0.08739485 -1.88706386 -0.5208651   0.6220278   0.28155814\n",
      "   1.10665558 -0.63320325  1.83466919]]\n"
     ]
    }
   ],
   "source": [
    "# функции для нахождения среднего и стандартного распределения\n",
    "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6457ca",
   "metadata": {},
   "source": [
    "Также нужно добавить единичный столбец к данным, чтобы был свободный параметр ```w0```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "42f0b102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.86580164  0.12296253  1.55757652  0.91545987 -1.66258049  0.94244347\n",
      "   0.9941688  -1.24316968  0.76671086  1.        ]\n",
      " [-0.79287009  0.94101901  0.35944073  2.02032522 -1.43859928 -0.27652282\n",
      "   0.80669085 -0.26936362  1.51428169  1.        ]\n",
      " [-0.53760968  1.4745341  -1.13822899  0.47351372  1.02519397  0.13469473\n",
      "  -0.05570773 -0.94353704  1.39583541  1.        ]\n",
      " [-0.61054123  1.86577851 -0.38939413  1.0259464  -0.18430453 -0.54087695\n",
      "  -1.29306223  1.18599488  1.52399041  1.        ]\n",
      " [-0.06355463  0.12296253  0.95850863  1.13643294 -0.3634895   0.06126302\n",
      "   0.09427463 -0.95423821  0.74729344  1.        ]\n",
      " [-0.647007    0.30080089 -0.68892807  0.47351372 -0.90104439 -0.3058955\n",
      "   0.16926581 -0.39777761  0.11428541  1.        ]\n",
      " [-0.75640432  0.97658668 -1.28799597  0.69448679  0.93560149 -0.39401355\n",
      "  -0.46815923  0.22288999  0.52981829  1.        ]\n",
      " [-0.53760968  0.30080089 -0.5391611   0.80497333  0.03967667  0.47248057\n",
      "   0.35674376 -0.53689276 -0.17115072  1.        ]\n",
      " [-0.53760968  0.51420693 -1.28799597  0.14205412  1.1595827   0.25218545\n",
      "   0.50672613 -0.11954731 -0.1517333   1.        ]\n",
      " [-0.50114391  0.47863926  1.55757652  0.36302719 -0.90104439  5.21616867\n",
      "   1.93155857 -1.33948017 -1.24881777  1.        ]\n",
      " [-0.79287009  0.72761297  1.40780954  1.0259464  -0.63226694 -0.32058184\n",
      "   0.73169967 -0.04463915 -1.0216339   1.        ]\n",
      " [-0.2458835   0.1585302   0.65897468  0.14205412  0.6220278   0.72214836\n",
      "   0.58171731 -0.95423821 -0.7789161   1.        ]\n",
      " [-0.57407545  0.83431599  0.05990679  0.03156758 -0.81145191 -0.10028673\n",
      "   0.65670849 -0.7723184  -0.45464512  1.        ]\n",
      " [-0.50114391  0.47863926 -0.38939413  0.47351372 -0.31869326  0.29624448\n",
      "   0.01928345 -1.26457201  0.72205079  1.        ]\n",
      " [-0.68347277  1.36783109 -0.68892807  0.36302719  0.80121277 -0.17371843\n",
      "   0.73169967 -0.32286945  0.77641958  1.        ]\n",
      " [-0.75640432  1.36783109  0.35944073  0.80497333  1.1595827   0.51653959\n",
      "   1.10665558 -1.64981396  0.09292624  1.        ]\n",
      " [-0.647007    0.62090995 -0.08986018  1.13643294 -2.46891283  0.22281277\n",
      "   0.31924817 -1.10405453  0.78030306  1.        ]\n",
      " [ 0.08230846 -0.90849999 -1.13822899 -0.8523247  -0.31869326 -0.65836768\n",
      "  -1.06808868  1.23950071  0.7822448   1.        ]\n",
      " [-0.13648618 -0.51725559 -0.23962716 -1.07329777 -1.03543311 -0.48213159\n",
      "  -0.73062837  0.32990164  0.16477071  1.        ]\n",
      " [ 0.3011031  -0.90849999 -0.08986018 -1.18378431 -0.63226694 -0.29120916\n",
      "  -0.91810632 -0.4619846  -0.04105398  1.        ]\n",
      " [-0.46467813  0.26523322 -1.43776294 -0.41037856 -0.18430453 -0.40869989\n",
      "  -0.91810632  0.04097018  0.12593587  1.        ]\n",
      " [-0.50114391  0.6920453  -0.68892807 -0.29989203 -0.49787822  0.23749911\n",
      "   1.36912471 -1.02914637  0.12399412  1.        ]\n",
      " [ 0.48343196 -0.41055257  1.25804257 -0.29989203  0.53243532 -0.52619061\n",
      "  -0.580646    1.57123683 -0.70901337  1.        ]\n",
      " [ 4.53113275 -2.86472201 -0.38939413 -3.28302849  0.03967667 -1.15770327\n",
      "  -2.49292113  1.05758089 -2.01192254  1.        ]\n",
      " [ 1.57740515 -1.83325949 -0.08986018 -0.96281124 -1.3490068  -0.54087695\n",
      "  -1.63052254  1.16459255 -1.65075845  1.        ]\n",
      " [ 1.28567896 -1.86882717 -0.23962716 -1.07329777 -0.54267446 -0.61430866\n",
      "  -1.44304459  1.50702984 -1.52260345  1.        ]\n",
      " [ 1.43154205 -0.90849999 -0.83869505 -1.84670352  0.12926915 -0.54087695\n",
      "  -1.66801813  1.19669604 -1.43910852  1.        ]\n",
      " [ 0.22817155 -0.09044351  2.45617835  0.80497333  0.75641653  0.14938107\n",
      "   0.54422172  0.08377484 -0.2798883   1.        ]\n",
      " [ 0.48343196 -0.73066163  1.40780954  0.25254065  1.87632255 -0.20309111\n",
      "   0.31924817  0.85425875 -0.74590648  1.        ]\n",
      " [-0.06355463  0.08739485  1.25804257  0.58400026  0.53243532 -0.27652282\n",
      "  -0.2056901   0.19078649 -0.41192678  1.        ]\n",
      " [ 0.66576082 -1.01520301  0.20967376 -0.63135163  0.35325036 -0.10028673\n",
      "   1.70658503  0.08377484 -1.26240996  1.        ]\n",
      " [ 0.00937691 -0.65952628 -0.68892807 -0.96281124  0.3980466  -0.10028673\n",
      "  -0.05570773 -0.59039858  0.39972155  1.        ]\n",
      " [-0.13648618 -0.33941722  0.20967376  0.03156758  2.27948872 -0.76117207\n",
      "  -0.50565482  2.00998462  1.84631965  1.        ]\n",
      " [ 0.33756887 -0.73066163  0.50920771 -0.29989203  0.98039773 -0.73179939\n",
      "  -0.01821214  1.8922718  -0.34008231  1.        ]\n",
      " [-0.06355463  0.08739485 -1.88706386 -0.5208651   0.6220278   0.28155814\n",
      "   1.10665558 -0.63320325  1.83466919  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.append(X_train, np.ones(X_train.shape[0]).reshape((X_train.shape[0], 1)), axis=1)\n",
    "X_test = np.append(X_test, np.ones(X_test.shape[0]).reshape((X_test.shape[0], 1)), axis=1)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8666173",
   "metadata": {},
   "source": [
    "### 2. Расчет весов линейной регрессии по аналитической формуле.\n",
    "\n",
    "После нормализации можно рассчитать веса с помощью аналитической формулы (здесь также будут использоваться функции numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "415bcc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полученные веса:\n",
      "[[-1.56018400e-01]\n",
      " [-2.23529642e+00]\n",
      " [ 6.68635399e-01]\n",
      " [-8.35658594e-03]\n",
      " [ 4.98044131e-01]\n",
      " [ 3.28450435e-01]\n",
      " [-6.46361963e-01]\n",
      " [ 1.75903141e-01]\n",
      " [ 7.07629979e-01]\n",
      " [ 5.92542857e+01]]\n"
     ]
    }
   ],
   "source": [
    "cov_mat = np.matmul(np.transpose(X_train), X_train) # получение ковариационной матрицы\n",
    "temp = np.matmul(np.linalg.inv(cov_mat), np.transpose(X_train)) # произведение обратной ковариационной матрицы и X транспонированной\n",
    "w = np.matmul(temp, y_train) # получение матрицы весов\n",
    "\n",
    "print(f\"Полученные веса:\\n{w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7b7f93",
   "metadata": {},
   "source": [
    "### 3. Построение и интепретация корреляционной матрицы. Определение степени мультиколлинеарности на основе числа обусловленности.\n",
    "\n",
    "Теперь определим степень мультиколлинеарности на основе числа обусловленности. Будет использоваться функция ```eigvals``` из linalg для подсчета собственных значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b2db269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ковариационная матрица:\n",
      "[[ 3.50000000e+01 -2.98986542e+01 -4.07418083e-01 -2.83018361e+01\n",
      "   2.31475804e+00 -1.14787075e+01 -2.20240523e+01  1.81055400e+01\n",
      "  -2.13634138e+01  7.16093851e-15]\n",
      " [-2.98986542e+01  3.50000000e+01 -1.96774309e+00  2.76985550e+01\n",
      "   9.36586120e-01  1.03188134e+01  1.89319196e+01 -1.90080387e+01\n",
      "   2.10393130e+01  1.17822418e-14]\n",
      " [-4.07418083e-01 -1.96774309e+00  3.50000000e+01  1.16823472e+01\n",
      "  -1.52562601e+00  1.04072905e+01  1.01574993e+01 -5.58694650e-01\n",
      "  -1.06026011e+01  3.79696274e-14]\n",
      " [-2.83018361e+01  2.76985550e+01  1.16823472e+01  3.50000000e+01\n",
      "  -2.82044039e+00  1.00640950e+01  2.05847723e+01 -1.51980680e+01\n",
      "   1.78342877e+01  4.32986980e-15]\n",
      " [ 2.31475804e+00  9.36586120e-01 -1.52562601e+00 -2.82044039e+00\n",
      "   3.50000000e+01 -6.13768931e+00  5.69501235e-01  1.05578393e+01\n",
      "   2.69406736e+00 -3.91908728e-14]\n",
      " [-1.14787075e+01  1.03188134e+01  1.04072905e+01  1.00640950e+01\n",
      "  -6.13768931e+00  3.50000000e+01  2.10598920e+01 -1.97868751e+01\n",
      "  -2.67750227e+00  3.99680289e-15]\n",
      " [-2.20240523e+01  1.89319196e+01  1.01574993e+01  2.05847723e+01\n",
      "   5.69501235e-01  2.10598920e+01  3.50000000e+01 -2.30716405e+01\n",
      "   7.13481471e+00 -1.31006317e-14]\n",
      " [ 1.81055400e+01 -1.90080387e+01 -5.58694650e-01 -1.51980680e+01\n",
      "   1.05578393e+01 -1.97868751e+01 -2.30716405e+01  3.50000000e+01\n",
      "  -8.20259519e+00  6.10622664e-15]\n",
      " [-2.13634138e+01  2.10393130e+01 -1.06026011e+01  1.78342877e+01\n",
      "   2.69406736e+00 -2.67750227e+00  7.13481471e+00 -8.20259519e+00\n",
      "   3.50000000e+01  2.22044605e-15]\n",
      " [ 7.16093851e-15  1.17822418e-14  3.79696274e-14  4.32986980e-15\n",
      "  -3.91908728e-14  3.99680289e-15 -1.31006317e-14  6.10622664e-15\n",
      "   2.22044605e-15  3.50000000e+01]]\n",
      "Число обусловленности: 6.790224663844371\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ковариационная матрица:\\n{cov_mat}\")\n",
    "print(f\"Число обусловленности: {np.linalg.cond(X_train, p = 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa2ea57",
   "metadata": {},
   "source": [
    "### 4. Анализ регрессионных остатков.\n",
    "\n",
    "Найдем среднеквадратическую ошибку (MSE) и коэффициент детерминации (r2) для матрицы ```X_train```, будем искать регрессионные остатки относительно ```y_train```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fa14ff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.4463814111475926\n",
      "R2: 0.6766573558026336\n"
     ]
    }
   ],
   "source": [
    "# оценочные значения целевого набора обучающих данных\n",
    "y_pred_train = np.matmul(X_train, w)\n",
    "\n",
    "# MSE для обучающий данных\n",
    "Eps_train = y_pred_train - y_train\n",
    "MSE_train = np.matmul(np.transpose(Eps_train), Eps_train) / X_train.shape[0]\n",
    "\n",
    "# коэффициент детерминации для обучающих данных\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"MSE: {MSE_train[0][0]}\\nR2: {r2_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02b6ac",
   "metadata": {},
   "source": [
    "Теперь найдем MSE для валидационной выборки (```X_test``` и ```y_test```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bf63769e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 20.389900510037812\n"
     ]
    }
   ],
   "source": [
    "# оценочные значения целевого набора данных для валидации\n",
    "y_pred_test = np.matmul(X_test, w)\n",
    "\n",
    "# MSE для валидационных данных\n",
    "Eps_test = y_pred_test - y_test\n",
    "MSE_test = np.matmul(np.transpose(Eps_test), Eps_test) / X_test.shape[0]\n",
    "\n",
    "print(f\"MSE: {MSE_test[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf56f4",
   "metadata": {},
   "source": [
    "### 5. Определение весов линейной регрессии градиентным методом. Проанализировать изменение ошибки от итерации к итерации\n",
    "\n",
    "Сначала инициализурем весы (каждый параметр будет равен единице)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b6071b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Инициализация весов\n",
    "w = np.ones((X_train.shape[1], 1))\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451de50",
   "metadata": {},
   "source": [
    "Теперь циклично будем расчитывать оценочные значения по весам, градиент функции потерь и новые значения весов. Эти шаги будут выполняться на протяжении 500 итераций алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "948692ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1: 3414.5719234274197\n",
      "Эпоха 2: 2177.29186039666\n",
      "Эпоха 3: 1394.4373505344688\n",
      "Эпоха 4: 893.6640155037337\n",
      "Эпоха 5: 573.1673431870605\n",
      "Эпоха 6: 368.0374233041753\n",
      "Эпоха 7: 236.74062770154924\n",
      "Эпоха 8: 152.6966339480131\n",
      "Эпоха 9: 98.89458277709178\n",
      "Эпоха 10: 64.44780696269203\n",
      "Эпоха 11: 42.38897633708126\n",
      "Эпоха 12: 28.259054744532783\n",
      "Эпоха 13: 19.204268989626303\n",
      "Эпоха 14: 13.398189872953749\n",
      "Эпоха 15: 9.67187602729905\n",
      "Эпоха 16: 7.277172784013111\n",
      "Эпоха 17: 5.735227264690987\n",
      "Эпоха 18: 4.739540120024771\n",
      "Эпоха 19: 4.093919718892148\n",
      "Эпоха 20: 3.672773330449116\n",
      "Эпоха 21: 3.3956936575330694\n",
      "Эпоха 22: 3.2111942171825314\n",
      "Эпоха 23: 3.0862998906177377\n",
      "Эпоха 24: 2.999884737833142\n",
      "Эпоха 25: 2.938408045290725\n",
      "Эпоха 26: 2.893185241697878\n",
      "Эпоха 27: 2.8586411361787847\n",
      "Эпоха 28: 2.8311918533602167\n",
      "Эпоха 29: 2.808529146481259\n",
      "Эпоха 30: 2.789162244565751\n",
      "Эпоха 31: 2.772124532978606\n",
      "Эпоха 32: 2.7567857380693908\n",
      "Эпоха 33: 2.7427316441712755\n",
      "Эпоха 34: 2.729687040054409\n",
      "Эпоха 35: 2.717466340030466\n",
      "Эпоха 36: 2.70594192376026\n",
      "Эпоха 37: 2.6950238221663\n",
      "Эпоха 38: 2.6846466702790397\n",
      "Эпоха 39: 2.6747613157182366\n",
      "Эпоха 40: 2.6653294110219496\n",
      "Эпоха 41: 2.6563199193888374\n",
      "Эпоха 42: 2.6477068483239536\n",
      "Эпоха 43: 2.639467772082281\n",
      "Эпоха 44: 2.631582861548473\n",
      "Эпоха 45: 2.624034241188022\n",
      "Эпоха 46: 2.6168055573785183\n",
      "Эпоха 47: 2.609881683851541\n",
      "Эпоха 48: 2.603248516513114\n",
      "Эпоха 49: 2.5968928269182636\n",
      "Эпоха 50: 2.5908021545811706\n",
      "Эпоха 51: 2.584964725300381\n",
      "Эпоха 52: 2.579369387173332\n",
      "Эпоха 53: 2.57400555886529\n",
      "Эпоха 54: 2.5688631865600553\n",
      "Эпоха 55: 2.563932707222816\n",
      "Эпоха 56: 2.5592050165845923\n",
      "Эпоха 57: 2.5546714407649995\n",
      "Эпоха 58: 2.550323710781859\n",
      "Эпоха 59: 2.5461539394151096\n",
      "Эпоха 60: 2.5421546000382347\n",
      "Эпоха 61: 2.5383185071286363\n",
      "Эпоха 62: 2.534638798235834\n",
      "Эпоха 63: 2.5311089172332433\n",
      "Эпоха 64: 2.5277225987127774\n",
      "Эпоха 65: 2.5244738534060485\n",
      "Эпоха 66: 2.521356954534194\n",
      "Эпоха 67: 2.5183664250025335\n",
      "Эпоха 68: 2.5154970253672704\n",
      "Эпоха 69: 2.5127437425105903\n",
      "Эпоха 70: 2.5101017789677225\n",
      "Эпоха 71: 2.5075665428558405\n",
      "Эпоха 72: 2.505133638359928\n",
      "Эпоха 73: 2.5027988567351844\n",
      "Эпоха 74: 2.5005581677896247\n",
      "Эпоха 75: 2.498407711813773\n",
      "Эпоха 76: 2.4963437919275973\n",
      "Эпоха 77: 2.4943628668172764\n",
      "Эпоха 78: 2.492461543836945\n",
      "Эпоха 79: 2.4906365724525785\n",
      "Эпоха 80: 2.488884838007114\n",
      "Эпоха 81: 2.487203355787516\n",
      "Эпоха 82: 2.4855892653761718\n",
      "Эпоха 83: 2.4840398252701297\n",
      "Эпоха 84: 2.482552407753163\n",
      "Эпоха 85: 2.481124494006604\n",
      "Эпоха 86: 2.479753669445899\n",
      "Эпоха 87: 2.4784376192708937\n",
      "Эпоха 88: 2.47717412421843\n",
      "Эпоха 89: 2.4759610565068457\n",
      "Эпоха 90: 2.474796375962559\n",
      "Эпоха 91: 2.473678126319429\n",
      "Эпоха 92: 2.4726044316823734\n",
      "Эпоха 93: 2.471573493147135\n",
      "Эпоха 94: 2.47058358556851\n",
      "Эпоха 95: 2.4696330544700063\n",
      "Эпоха 96: 2.4687203130880406\n",
      "Эпоха 97: 2.4678438395444577\n",
      "Эпоха 98: 2.4670021741412462\n",
      "Эпоха 99: 2.4661939167718225\n",
      "Эпоха 100: 2.4654177244435296\n",
      "Эпоха 101: 2.464672308906196\n",
      "Эпоха 102: 2.4639564343820126\n",
      "Эпоха 103: 2.463268915392073\n",
      "Эпоха 104: 2.462608614675312\n",
      "Эпоха 105: 2.4619744411956495\n",
      "Эпоха 106: 2.4613653482334747\n",
      "Эпоха 107: 2.4607803315577095\n",
      "Эпоха 108: 2.4602184276749233\n",
      "Эпоха 109: 2.459678712152088\n",
      "Эпоха 110: 2.459160298009844\n",
      "Эпоха 111: 2.4586623341830864\n",
      "Эпоха 112: 2.4581840040460565\n",
      "Эпоха 113: 2.4577245239990995\n",
      "Эпоха 114: 2.4572831421144783\n",
      "Эпоха 115: 2.4568591368386117\n",
      "Эпоха 116: 2.4564518157484483\n",
      "Эпоха 117: 2.456060514359562\n",
      "Эпоха 118: 2.455684594983787\n",
      "Эпоха 119: 2.4553234456343227\n",
      "Эпоха 120: 2.4549764789762536\n",
      "Эпоха 121: 2.4546431313205557\n",
      "Эпоха 122: 2.4543228616597785\n",
      "Эпоха 123: 2.4540151507436296\n",
      "Эпоха 124: 2.453719500192744\n",
      "Эпоха 125: 2.4534354316491096\n",
      "Эпоха 126: 2.453162485961515\n",
      "Эпоха 127: 2.452900222404622\n",
      "Эпоха 128: 2.4526482179301947\n",
      "Эпоха 129: 2.452406066449198\n",
      "Эпоха 130: 2.4521733781434185\n",
      "Эпоха 131: 2.451949778805399\n",
      "Эпоха 132: 2.451734909205495\n",
      "Эпоха 133: 2.4515284244849638\n",
      "Эпоха 134: 2.4513299935739057\n",
      "Эпоха 135: 2.4511392986331204\n",
      "Эпоха 136: 2.450956034518867\n",
      "Эпоха 137: 2.4507799082694697\n",
      "Эпоха 138: 2.450610638613046\n",
      "Эпоха 139: 2.4504479554953056\n",
      "Эпоха 140: 2.4502915996266994\n",
      "Эпоха 141: 2.4501413220480526\n",
      "Эпоха 142: 2.4499968837139683\n",
      "Эпоха 143: 2.44985805509323\n",
      "Эпоха 144: 2.4497246157854913\n",
      "Эпоха 145: 2.449596354153635\n",
      "Эпоха 146: 2.449473066971091\n",
      "Эпоха 147: 2.4493545590835186\n",
      "Эпоха 148: 2.449240643084285\n",
      "Эпоха 149: 2.4491311390031467\n",
      "Эпоха 150: 2.4490258740075666\n",
      "Эпоха 151: 2.448924682116214\n",
      "Эпоха 152: 2.4488274039240707\n",
      "Эпоха 153: 2.448733886338723\n",
      "Эпоха 154: 2.448643982327348\n",
      "Эпоха 155: 2.4485575506739665\n",
      "Эпоха 156: 2.4484744557465206\n",
      "Эпоха 157: 2.448394567273437\n",
      "Эпоха 158: 2.448317760129196\n",
      "Эпоха 159: 2.448243914128581\n",
      "Эпоха 160: 2.4481729138292994\n",
      "Эпоха 161: 2.448104648342537\n",
      "Эпоха 162: 2.44803901115119\n",
      "Эпоха 163: 2.4479758999354453\n",
      "Эпоха 164: 2.4479152164054008\n",
      "Эпоха 165: 2.447856866140427\n",
      "Эпоха 166: 2.447800758435039\n",
      "Эпоха 167: 2.4477468061509327\n",
      "Эпоха 168: 2.4476949255750737\n",
      "Эпоха 169: 2.4476450362833875\n",
      "Эпоха 170: 2.4475970610100117\n",
      "Эпоха 171: 2.447550925521816\n",
      "Эпоха 172: 2.447506558497922\n",
      "Эпоха 173: 2.447463891414083\n",
      "Эпоха 174: 2.4474228584317457\n",
      "Эпоха 175: 2.4473833962915066\n",
      "Эпоха 176: 2.4473454442108844\n",
      "Эпоха 177: 2.4473089437862043\n",
      "Эпоха 178: 2.447273838898366\n",
      "Эпоха 179: 2.447240075622432\n",
      "Эпоха 180: 2.447207602140828\n",
      "Эпоха 181: 2.4471763686600156\n",
      "Эпоха 182: 2.447146327330485\n",
      "Эпоха 183: 2.4471174321699882\n",
      "Эпоха 184: 2.4470896389897794\n",
      "Эпоха 185: 2.447062905323864\n",
      "Эпоха 186: 2.447037190361048\n",
      "Эпоха 187: 2.44701245487967\n",
      "Эпоха 188: 2.4469886611850042\n",
      "Эпоха 189: 2.4469657730490937\n",
      "Эпоха 190: 2.446943755653027\n",
      "Эпоха 191: 2.446922575531483\n",
      "Эпоха 192: 2.446902200519503\n",
      "Эпоха 193: 2.446882599701376\n",
      "Эпоха 194: 2.44686374336156\n",
      "Эпоха 195: 2.4468456029375605\n",
      "Эпоха 196: 2.446828150974631\n",
      "Эпоха 197: 2.4468113610823745\n",
      "Эпоха 198: 2.4467952078929667\n",
      "Эпоха 199: 2.4467796670210853\n",
      "Эпоха 200: 2.4467647150254335\n",
      "Эпоха 201: 2.446750329371761\n",
      "Эпоха 202: 2.4467364883973763\n",
      "Эпоха 203: 2.446723171277045\n",
      "Эпоха 204: 2.4467103579902543\n",
      "Эпоха 205: 2.4466980292897658\n",
      "Эпоха 206: 2.4466861666713924\n",
      "Эпоха 207: 2.4466747523450105\n",
      "Эпоха 208: 2.4466637692066846\n",
      "Эпоха 209: 2.4466532008119035\n",
      "Эпоха 210: 2.4466430313498684\n",
      "Эпоха 211: 2.4466332456188047\n",
      "Эпоха 212: 2.4466238290022275\n",
      "Эпоха 213: 2.446614767446179\n",
      "Эпоха 214: 2.4466060474373164\n",
      "Эпоха 215: 2.446597655981906\n",
      "Эпоха 216: 2.446589580585594\n",
      "Эпоха 217: 2.446581809234042\n",
      "Эпоха 218: 2.446574330374232\n",
      "Эпоха 219: 2.4465671328966105\n",
      "Эпоха 220: 2.446560206117849\n",
      "Эпоха 221: 2.44655353976432\n",
      "Эпоха 222: 2.446547123956207\n",
      "Эпоха 223: 2.446540949192271\n",
      "Эпоха 224: 2.446535006335154\n",
      "Эпоха 225: 2.44652928659731\n",
      "Эпоха 226: 2.4465237815274667\n",
      "Эпоха 227: 2.4465184829976208\n",
      "Эпоха 228: 2.4465133831905406\n",
      "Эпоха 229: 2.4465084745877554\n",
      "Эпоха 230: 2.4465037499580085\n",
      "Эпоха 231: 2.446499202346179\n",
      "Эпоха 232: 2.44649482506263\n",
      "Эпоха 233: 2.446490611672949\n",
      "Эпоха 234: 2.446486555988138\n",
      "Эпоха 235: 2.4464826520551157\n",
      "Эпоха 236: 2.446478894147676\n",
      "Эпоха 237: 2.446475276757711\n",
      "Эпоха 238: 2.446471794586844\n",
      "Эпоха 239: 2.4464684425383507\n",
      "Эпоха 240: 2.446465215709412\n",
      "Эпоха 241: 2.4464621093836505\n",
      "Эпоха 242: 2.446459119023982\n",
      "Эпоха 243: 2.446456240265714\n",
      "Эпоха 244: 2.446453468909948\n",
      "Эпоха 245: 2.4464508009172055\n",
      "Эпоха 246: 2.4464482324013113\n",
      "Эпоха 247: 2.4464457596235296\n",
      "Эпоха 248: 2.446443378986905\n",
      "Эпоха 249: 2.4464410870308386\n",
      "Эпоха 250: 2.446438880425856\n",
      "Эпоха 251: 2.4464367559686058\n",
      "Эпоха 252: 2.4464347105770203\n",
      "Эпоха 253: 2.4464327412856863\n",
      "Эпоха 254: 2.4464308452413928\n",
      "Эпоха 255: 2.446429019698831\n",
      "Эпоха 256: 2.446427262016479\n",
      "Эпоха 257: 2.4464255696526385\n",
      "Эпоха 258: 2.4464239401616443\n",
      "Эпоха 259: 2.4464223711901694\n",
      "Эпоха 260: 2.4464208604737196\n",
      "Эпоха 261: 2.4464194058332698\n",
      "Эпоха 262: 2.4464180051719753\n",
      "Эпоха 263: 2.446416656472054\n",
      "Эпоха 264: 2.446415357791783\n",
      "Эпоха 265: 2.4464141072626076\n",
      "Эпоха 266: 2.4464129030863466\n",
      "Эпоха 267: 2.4464117435325208\n",
      "Эпоха 268: 2.4464106269357924\n",
      "Эпоха 269: 2.446409551693471\n",
      "Эпоха 270: 2.4464085162631446\n",
      "Эпоха 271: 2.4464075191604038\n",
      "Эпоха 272: 2.446406558956623\n",
      "Эпоха 273: 2.446405634276851\n",
      "Эпоха 274: 2.44640474379778\n",
      "Эпоха 275: 2.4464038862457786\n",
      "Эпоха 276: 2.446403060395024\n",
      "Эпоха 277: 2.446402265065688\n",
      "Эпоха 278: 2.4464014991221794\n",
      "Эпоха 279: 2.446400761471502\n",
      "Эпоха 280: 2.4464000510616146\n",
      "Эпоха 281: 2.446399366879889\n",
      "Эпоха 282: 2.4463987079516443\n",
      "Эпоха 283: 2.4463980733386737\n",
      "Эпоха 284: 2.446397462137898\n",
      "Эпоха 285: 2.446396873480026\n",
      "Эпоха 286: 2.4463963065282903\n",
      "Эпоха 287: 2.446395760477199\n",
      "Эпоха 288: 2.4463952345513844\n",
      "Эпоха 289: 2.4463947280044427\n",
      "Эпоха 290: 2.44639424011786\n",
      "Эпоха 291: 2.4463937701999563\n",
      "Эпоха 292: 2.4463933175848673\n",
      "Эпоха 293: 2.446392881631586\n",
      "Эпоха 294: 2.446392461723014\n",
      "Эпоха 295: 2.446392057265081\n",
      "Эпоха 296: 2.4463916676858473\n",
      "Эпоха 297: 2.4463912924347193\n",
      "Эпоха 298: 2.4463909309815963\n",
      "Эпоха 299: 2.4463905828161376\n",
      "Эпоха 300: 2.446390247446998\n",
      "Эпоха 301: 2.4463899244011396\n",
      "Эпоха 302: 2.4463896132231095\n",
      "Эпоха 303: 2.4463893134744206\n",
      "Эпоха 304: 2.446389024732878\n",
      "Эпоха 305: 2.4463887465920013\n",
      "Эпоха 306: 2.446388478660409\n",
      "Эпоха 307: 2.446388220561275\n",
      "Эпоха 308: 2.4463879719317565\n",
      "Эпоха 309: 2.446387732422504\n",
      "Эпоха 310: 2.4463875016971275\n",
      "Эпоха 311: 2.446387279431731\n",
      "Эпоха 312: 2.4463870653144406\n",
      "Эпоха 313: 2.446386859044936\n",
      "Эпоха 314: 2.446386660334055\n",
      "Эпоха 315: 2.4463864689033468\n",
      "Эпоха 316: 2.4463862844846767\n",
      "Эпоха 317: 2.4463861068198636\n",
      "Эпоха 318: 2.4463859356602744\n",
      "Эпоха 319: 2.446385770766489\n",
      "Эпоха 320: 2.446385611907957\n",
      "Эпоха 321: 2.446385458862656\n",
      "Эпоха 322: 2.4463853114167873\n",
      "Эпоха 323: 2.4463851693644605\n",
      "Эпоха 324: 2.4463850325073913\n",
      "Эпоха 325: 2.446384900654634\n",
      "Эпоха 326: 2.446384773622308\n",
      "Эпоха 327: 2.446384651233319\n",
      "Эпоха 328: 2.446384533317116\n",
      "Эпоха 329: 2.446384419709454\n",
      "Эпоха 330: 2.446384310252155\n",
      "Эпоха 331: 2.4463842047928734\n",
      "Эпоха 332: 2.446384103184889\n",
      "Эпоха 333: 2.446384005286905\n",
      "Эпоха 334: 2.4463839109628145\n",
      "Эпоха 335: 2.4463838200815635\n",
      "Эпоха 336: 2.4463837325168987\n",
      "Эпоха 337: 2.4463836481472416\n",
      "Эпоха 338: 2.4463835668554914\n",
      "Эпоха 339: 2.446383488528853\n",
      "Эпоха 340: 2.446383413058704\n",
      "Эпоха 341: 2.4463833403403985\n",
      "Эпоха 342: 2.4463832702731616\n",
      "Эпоха 343: 2.446383202759924\n",
      "Эпоха 344: 2.446383137707191\n",
      "Эпоха 345: 2.4463830750248974\n",
      "Эпоха 346: 2.44638301462631\n",
      "Эпоха 347: 2.446382956427865\n",
      "Эпоха 348: 2.4463829003490907\n",
      "Эпоха 349: 2.4463828463124564\n",
      "Эпоха 350: 2.446382794243291\n",
      "Эпоха 351: 2.446382744069667\n",
      "Эпоха 352: 2.446382695722293\n",
      "Эпоха 353: 2.446382649134432\n",
      "Эпоха 354: 2.4463826042417893\n",
      "Эпоха 355: 2.446382560982438\n",
      "Эпоха 356: 2.446382519296721\n",
      "Эпоха 357: 2.446382479127163\n",
      "Эпоха 358: 2.4463824404184082\n",
      "Эпоха 359: 2.446382403117129\n",
      "Эпоха 360: 2.4463823671719487\n",
      "Эпоха 361: 2.446382332533383\n",
      "Эпоха 362: 2.446382299153754\n",
      "Эпоха 363: 2.4463822669871336\n",
      "Эпоха 364: 2.4463822359892795\n",
      "Эпоха 365: 2.4463822061175633\n",
      "Эпоха 366: 2.4463821773309258\n",
      "Эпоха 367: 2.446382149589809\n",
      "Эпоха 368: 2.4463821228561007\n",
      "Эпоха 369: 2.4463820970930885\n",
      "Эпоха 370: 2.4463820722653953\n",
      "Эпоха 371: 2.4463820483389522\n",
      "Эпоха 372: 2.446382025280926\n",
      "Эпоха 373: 2.446382003059688\n",
      "Эпоха 374: 2.446381981644763\n",
      "Эпоха 375: 2.4463819610067987\n",
      "Эпоха 376: 2.4463819411175014\n",
      "Эпоха 377: 2.4463819219496323\n",
      "Эпоха 378: 2.4463819034769254\n",
      "Эпоха 379: 2.446381885674088\n",
      "Эпоха 380: 2.4463818685167555\n",
      "Эпоха 381: 2.446381851981442\n",
      "Эпоха 382: 2.446381836045519\n",
      "Эпоха 383: 2.4463818206871983\n",
      "Эпоха 384: 2.4463818058854674\n",
      "Эпоха 385: 2.446381791620101\n",
      "Эпоха 386: 2.446381777871595\n",
      "Эпоха 387: 2.4463817646211696\n",
      "Эпоха 388: 2.446381751850717\n",
      "Эпоха 389: 2.446381739542809\n",
      "Эпоха 390: 2.446381727680628\n",
      "Эпоха 391: 2.4463817162479953\n",
      "Эпоха 392: 2.4463817052293018\n",
      "Эпоха 393: 2.446381694609515\n",
      "Эпоха 394: 2.446381684374155\n",
      "Эпоха 395: 2.4463816745092615\n",
      "Эпоха 396: 2.4463816650013954\n",
      "Эпоха 397: 2.446381655837585\n",
      "Эпоха 398: 2.446381647005356\n",
      "Эпоха 399: 2.4463816384926664\n",
      "Эпоха 400: 2.4463816302879304\n",
      "Эпоха 401: 2.4463816223799744\n",
      "Эпоха 402: 2.4463816147580313\n",
      "Эпоха 403: 2.4463816074117295\n",
      "Эпоха 404: 2.4463816003310765\n",
      "Эпоха 405: 2.4463815935064366\n",
      "Эпоха 406: 2.4463815869285255\n",
      "Эпоха 407: 2.446381580588407\n",
      "Эпоха 408: 2.446381574477461\n",
      "Эпоха 409: 2.4463815685873755\n",
      "Эпоха 410: 2.4463815629101493\n",
      "Эпоха 411: 2.4463815574380834\n",
      "Эпоха 412: 2.4463815521637238\n",
      "Эпоха 413: 2.4463815470799193\n",
      "Эпоха 414: 2.446381542179775\n",
      "Эпоха 415: 2.4463815374566256\n",
      "Эпоха 416: 2.4463815329040703\n",
      "Эпоха 417: 2.446381528515934\n",
      "Эпоха 418: 2.4463815242862617\n",
      "Эпоха 419: 2.446381520209311\n",
      "Эпоха 420: 2.446381516279564\n",
      "Эпоха 421: 2.44638151249169\n",
      "Эпоха 422: 2.4463815088405494\n",
      "Эпоха 423: 2.4463815053212055\n",
      "Эпоха 424: 2.446381501928875\n",
      "Эпоха 425: 2.4463814986589805\n",
      "Эпоха 426: 2.4463814955070804\n",
      "Эпоха 427: 2.4463814924689116\n",
      "Эпоха 428: 2.4463814895403586\n",
      "Эпоха 429: 2.4463814867174656\n",
      "Эпоха 430: 2.4463814839964035\n",
      "Эпоха 431: 2.4463814813734923\n",
      "Эпоха 432: 2.446381478845186\n",
      "Эпоха 433: 2.446381476408065\n",
      "Эпоха 434: 2.4463814740588314\n",
      "Эпоха 435: 2.4463814717943104\n",
      "Эпоха 436: 2.446381469611444\n",
      "Эпоха 437: 2.446381467507276\n",
      "Эпоха 438: 2.4463814654789706\n",
      "Эпоха 439: 2.446381463523776\n",
      "Эпоха 440: 2.446381461639063\n",
      "Эпоха 441: 2.4463814598222786\n",
      "Эпоха 442: 2.4463814580709684\n",
      "Эпоха 443: 2.446381456382781\n",
      "Эпоха 444: 2.44638145475542\n",
      "Эпоха 445: 2.4463814531866954\n",
      "Эпоха 446: 2.4463814516744935\n",
      "Эпоха 447: 2.446381450216773\n",
      "Эпоха 448: 2.446381448811567\n",
      "Эпоха 449: 2.44638144745698\n",
      "Эпоха 450: 2.446381446151185\n",
      "Эпоха 451: 2.446381444892419\n",
      "Эпоха 452: 2.446381443678989\n",
      "Эпоха 453: 2.4463814425092547\n",
      "Эпоха 454: 2.446381441381642\n",
      "Эпоха 455: 2.4463814402946342\n",
      "Эпоха 456: 2.446381439246763\n",
      "Эпоха 457: 2.4463814382366174\n",
      "Эпоха 458: 2.446381437262836\n",
      "Эпоха 459: 2.446381436324107\n",
      "Эпоха 460: 2.4463814354191733\n",
      "Эпоха 461: 2.4463814345468116\n",
      "Эпоха 462: 2.4463814337058434\n",
      "Эпоха 463: 2.4463814328951408\n",
      "Эпоха 464: 2.446381432113613\n",
      "Эпоха 465: 2.446381431360208\n",
      "Эпоха 466: 2.4463814306339144\n",
      "Эпоха 467: 2.4463814299337474\n",
      "Эпоха 468: 2.446381429258778\n",
      "Эпоха 469: 2.4463814286080856\n",
      "Эпоха 470: 2.4463814279808003\n",
      "Эпоха 471: 2.4463814273760827\n",
      "Эпоха 472: 2.4463814267931188\n",
      "Эпоха 473: 2.446381426231115\n",
      "Эпоха 474: 2.446381425689327\n",
      "Эпоха 475: 2.446381425167025\n",
      "Эпоха 476: 2.446381424663504\n",
      "Эпоха 477: 2.4463814241780892\n",
      "Эпоха 478: 2.44638142371013\n",
      "Эпоха 479: 2.446381423258993\n",
      "Эпоха 480: 2.4463814228240817\n",
      "Эпоха 481: 2.4463814224047984\n",
      "Эпоха 482: 2.446381422000591\n",
      "Эпоха 483: 2.4463814216109157\n",
      "Эпоха 484: 2.446381421235249\n",
      "Эпоха 485: 2.44638142087308\n",
      "Эпоха 486: 2.446381420523933\n",
      "Эпоха 487: 2.4463814201873317\n",
      "Эпоха 488: 2.4463814198628286\n",
      "Эпоха 489: 2.446381419549986\n",
      "Эпоха 490: 2.4463814192483837\n",
      "Эпоха 491: 2.446381418957618\n",
      "Эпоха 492: 2.4463814186773054\n",
      "Эпоха 493: 2.4463814184070625\n",
      "Эпоха 494: 2.4463814181465255\n",
      "Эпоха 495: 2.4463814178953527\n",
      "Эпоха 496: 2.4463814176531997\n",
      "Эпоха 497: 2.4463814174197487\n",
      "Эпоха 498: 2.446381417194679\n",
      "Эпоха 499: 2.446381416977696\n",
      "Эпоха 500: 2.446381416768505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "S = []\n",
    "\n",
    "for i in range(500):\n",
    "    # 2. Расчет предсказанного значения y_pred по весам w\n",
    "    y_pred = np.matmul(X_train, w)\n",
    "    Eps_train = y_train - y_pred\n",
    "\n",
    "    # 3. Расчет ошибки и градиента функции потерь\n",
    "    S.append((1 / X_train.shape[0]) * np.matmul(np.transpose(Eps_train), Eps_train)[0][0])\n",
    "\n",
    "    # 4. Установка новых значений весов\n",
    "    dS = (-2 / X_train.shape[0]) * np.transpose(np.matmul(np.transpose(Eps_train), X_train))\n",
    "    w -= learning_rate * dS\n",
    "\n",
    "[print(f\"Эпоха {_ + 1}: {S[_]}\") for _ in range(len(S))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1272f971",
   "metadata": {},
   "source": [
    "После использования градиентного спуска можно расчитать MSE для набора тестовых данных ```X_test``` и ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e5326a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 20.389900510037812\n"
     ]
    }
   ],
   "source": [
    "Eps_test = y_test - np.matmul(X_test, w) # регресионный остатки\n",
    "MSE_test = (1 / X_test.shape[0]) * np.matmul(np.transpose(Eps_test), Eps_test) # среднеквадратичная ошибка\n",
    "\n",
    "print(f\"MSE: {MSE_test[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70578c",
   "metadata": {},
   "source": [
    "Также найдем MSE и R2 для обучающих данных ```X_train``` и ```y_train```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "318b6d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.4463814165668283\n",
      "R2: 0.6766573550863635\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = np.matmul(X_train, w)\n",
    "\n",
    "Eps_train = y_train - y_pred_train\n",
    "MSE_train = (1 / X_train.shape[0]) * np.matmul(np.transpose(Eps_train), Eps_train)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"MSE: {MSE_train[0][0]}\\nR2: {r2_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee37b45",
   "metadata": {},
   "source": [
    "### 6. Сравнение результатов по аналитическому и градиентному методу.\n",
    "\n",
    "Если сравнивать ошибки на тестовых данных по аналитическому и градиентному методу, то можно увидеть, что результаты для первого и второго методов примерно одинаковые."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c6c84",
   "metadata": {},
   "source": [
    "### 7. С помощью библиотеки sklearn сделать fit-predict модели линейной регрессии. Сравнить результаты с ранее полученными.\n",
    "\n",
    "Теперь создадим модель через scikit-learn. Для этого воспользуемся одноименной библиотеки и функцией ```LinearRegression```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d9d74284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 20.389912982292394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# инициализация модели\n",
    "model = LinearRegression()\n",
    "\n",
    "# обучение модели\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# получение оценочных значений\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# регрессионные остатки и MSE\n",
    "Eps_test = y_test - y_pred\n",
    "MSE_test = (1 / X_test.shape[0]) * np.matmul(np.transpose(Eps_test), Eps_test)\n",
    "\n",
    "print(f\"MSE: {MSE_test[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe48903",
   "metadata": {},
   "source": [
    "Для полученной модели найдем MSE и R2 для ```X_train``` и ```y_train```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b4e549c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.446381411147593\n",
      "R2: 0.6766573558026336\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "Eps_train = y_train - y_pred_train\n",
    "MSE_train = (1 / X_train.shape[0]) * np.matmul(np.transpose(Eps_train), Eps_train)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"MSE: {MSE_train[0][0]}\\nR2: {r2_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d8f384",
   "metadata": {},
   "source": [
    "### 8. С помощью библиотеки statmodels получить «эконометрический» результат обучения модели линейной регрессии. Проинтерпретировать все его составляющие (в т.ч. те, которые изучались только теоретически), сравнить с предыдущими результатами.\n",
    "\n",
    "Здесь используем библиотеку statsmodel, а именно statsmodel.api. Для обучения будет использоваться метод ```fit```, а для получения отчета - ```summary```. Модель для обучения - ```OLS```, Ordinary Linear Regression (Обыкновенная линейная регрессия), которая использовалась в предыдущих заданиях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c7c306ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.677\n",
      "Model:                            OLS   Adj. R-squared:                  0.560\n",
      "Method:                 Least Squares   F-statistic:                     5.813\n",
      "Date:                Tue, 03 Oct 2023   Prob (F-statistic):           0.000226\n",
      "Time:                        22:32:43   Log-Likelihood:                -65.319\n",
      "No. Observations:                  35   AIC:                             150.6\n",
      "Df Residuals:                      25   BIC:                             166.2\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.1560      0.760     -0.205      0.839      -1.722       1.410\n",
      "x2            -2.2353      0.764     -2.926      0.007      -3.809      -0.662\n",
      "x3             0.6686      0.488      1.370      0.183      -0.337       1.674\n",
      "x4            -0.0084      0.791     -0.011      0.992      -1.637       1.620\n",
      "x5             0.4980      0.379      1.315      0.200      -0.282       1.278\n",
      "x6             0.3285      0.443      0.741      0.465      -0.584       1.241\n",
      "x7            -0.6464      0.583     -1.109      0.278      -1.846       0.554\n",
      "x8             0.1759      0.531      0.331      0.743      -0.918       1.270\n",
      "x9             0.7076      0.475      1.489      0.149      -0.271       1.687\n",
      "const         59.2543      0.313    189.421      0.000      58.610      59.899\n",
      "==============================================================================\n",
      "Omnibus:                       15.657   Durbin-Watson:                   2.621\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               25.905\n",
      "Skew:                           1.038   Prob(JB):                     2.37e-06\n",
      "Kurtosis:                       6.668   Cond. No.                         6.79\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# обучение модели\n",
    "results = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# получение отчета\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680edab",
   "metadata": {},
   "source": [
    "Из этого отчета можно увидеть, что коэффициент детерминации (```R-squared```) совпадает с тем, который был подсчитан для предыдущих моделей. А также, коэффициенты ```w0..w9``` примерно совпадают с теми, которые были подсчитаны аналитическим способом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c9282b",
   "metadata": {},
   "source": [
    "### 9. Сравнить качество получаемых моделей на основе коэффициента детерминации и MSE.\n",
    "\n",
    "Коэффициент детерминации R2 примерно одинаковый для всех моделей и равен ```0.677```, что близко к единице и говорит о том, что модель хорошо обучена. Для обучающего набора данных среднеквадратическая ошибка равна ```2.446```, это очень немного. А для валидационного набора ошибка составила ```20.389```, что намного больше. Это произошло из-за того, что эти данные никак не участвовали в алгоритмах расчета весов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebc852",
   "metadata": {},
   "source": [
    "### 10. Сделать итоговый вывод касательно причин различия в результатах при выполнении работ разными методами, а также по получаемым моделям в целом. Провести сравнительный анализ.\n",
    "\n",
    "Результаты примерно одинаковые для всех моделей, а именно MSE и R2 для обучающих данных и MSE для валидационных данных примерно совпадают."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
